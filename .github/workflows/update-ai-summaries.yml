name: AI Power BI Summarizer (V2)
on:
  schedule:
    - cron: '45 0 * * *'
  workflow_dispatch:

jobs:
  ai-summarize:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install Tools
        run: pip install google-generativeai requests beautifulsoup4

      - name: AI Processing
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          python - <<EOF
          import os, requests, time, google.generativeai as genai
          from bs4 import BeautifulSoup
          from datetime import datetime

          # 1. Configuration
          genai.configure(api_key=os.getenv("GEMINI_API_KEY"))
          model = genai.GenerativeModel('gemini-1.5-flash')
          HEADERS = {'User-Agent': 'Mozilla/5.0 (Linux; Android 14; SM-X910) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

          def fetch_summary(url):
              for attempt in range(3): # Try 3 times if it fails
                  try:
                      time.sleep(2) # Wait between requests
                      page = requests.get(url, headers=HEADERS, timeout=20)
                      page.raise_for_status()
                      soup = BeautifulSoup(page.text, 'html.parser')
                      
                      # Target the main blog content
                      content = soup.find('div', {'class': 'entry-content'}) or soup.find('article') or soup.body
                      text = content.get_text(separator=' ', strip=True)[:6000]
                      
                      prompt = f"Summarize this Power BI update into 4 clear, technical bullet points. Focus on new features and impact. Text: {text}"
                      return model.generate_content(prompt).text
                  except Exception as e:
                      if attempt == 2: return f"Summary unavailable: {str(e)}"
                      time.sleep(5)

          # 2. Get the latest 3 links from RSS
          try:
              feed = requests.get("https://powerbi.microsoft.com/en-us/blog/feed/", headers=HEADERS).text
              links = [l.split('</link>')[0] for l in feed.split('<link>')[1:4]]
          except:
              links = []

          # 3. Build the Section
          new_section = f"\n## ðŸ¤– AI Summary: {datetime.now().strftime('%Y-%m-%d')}\n"
          if not links:
              new_section += "No new links found today.\n"
          else:
              for url in links:
                  new_section += f"### [Original Post]({url})\n{fetch_summary(url)}\n\n---\n"

          # 4. Append and Keep History
          filename = "AI_SUMMARIES.md"
          existing_content = ""
          if os.path.exists(filename):
              with open(filename, "r") as f:
                  existing_content = f.read().replace("# Power BI AI Insight Lab", "")
          
          with open(filename, "w") as f:
              f.write("# Power BI AI Insight Lab\n" + new_section + existing_content)
          EOF

      - name: Save to GitHub
        run: |
          git config user.name "AI Assistant"
          git config user.email "ai@github.com"
          git add AI_SUMMARIES.md
          git commit -m "AI Insights Generated: $(date +'%Y-%m-%d')" || exit 0
          git push
